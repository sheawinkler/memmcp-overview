<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Context Lattice | Private memory fabric for AI agents</title>
  <link rel="stylesheet" href="styles.css?v=20260217a" />
</head>
<body class="home">
  <main class="container">
    <header class="topbar">
      <a class="brand" href="index.html">
        <span class="brand-mark" aria-hidden="true"></span>
        <span class="brand-label">Context Lattice</span>
      </a>
      <nav class="nav" aria-label="Primary">
        <a class="active" href="index.html">Home</a>
        <a href="updates.html">Updates</a>
        <a href="installation.html">Installation</a>
        <a href="troubleshooting.html">Troubleshooting</a>
        <a href="contact.html">Contact</a>
        <a href="v1/">v1</a>
      </nav>
    </header>

    <section class="hero">
      <span class="badge">Private memory layer for agents</span>
      <h1>Fix context drift before it burns your token budget.</h1>
      <p class="sub">
        Context Lattice (memMCP) gives agents reusable memory with HTTP MCP, federated retrieval,
        durable fanout, and storage guardrails so quality and costs remain stable under load.
      </p>
      <ul class="chips">
        <li>HTTP-only MCP</li>
        <li>Federated retrieval</li>
        <li>Durable outbox fanout</li>
        <li>Learning rerank schema</li>
        <li>Letta RAG support</li>
        <li>Retention controls</li>
      </ul>
      <div class="cta-row">
        <a class="cta primary" href="mailto:pilot@contextlattice.io?subject=Context%20Lattice%20Pilot">Book a pilot</a>
        <a class="cta secondary" href="updates.html">Read updates</a>
      </div>
      <div class="hero-art" aria-hidden="true">
        <span class="orb orb-a"></span>
        <span class="orb orb-b"></span>
        <span class="orb orb-c"></span>
        <span class="grid-sheen"></span>
      </div>
    </section>

    <div class="lattice-ribbon" aria-hidden="true">
      <span class="ribbon-wave ribbon-wave-a"></span>
      <span class="ribbon-wave ribbon-wave-b"></span>
      <span class="ribbon-dots"></span>
    </div>

    <section class="section">
      <div class="grid">
        <article class="card">
          <div class="kicker">What it solves</div>
          <h3>Memory quality + cost control</h3>
          <p>Replace repetitive long-context stuffing with reusable memory retrieval and sink durability.</p>
        </article>
        <article class="card">
          <div class="kicker">How it runs</div>
          <h3>Local first, enterprise path ready</h3>
          <p>Run on your machine now, then graduate to managed deployment with policy and access controls.</p>
        </article>
        <article class="card">
          <div class="kicker">Latest platform step</div>
          <h3>Self-protecting orchestrator</h3>
          <p>Fanout coalescer, Letta admission control, and retention workers reduce backlog and storage pressure.</p>
        </article>
      </div>
    </section>

    <section class="section">
      <article class="card">
        <div class="kicker">Learning retrieval</div>
        <h3>Orchestrator gets better at memory recall over time</h3>
        <p class="spotlight">
          The orchestrator uses a learning schema from feedback signals to rerank results and improve
          retrieval precision over time. This is reinforced by RAG through Letta archival memory,
          alongside Qdrant, Mongo raw, MindsDB, and memory-bank fallback.
        </p>
        <p class="muted-note">Read the detailed rollout notes on the <a href="updates.html">Updates page</a>.</p>
      </article>
    </section>

    <section class="section">
      <div class="kicker">How it all works together</div>
      <article class="card">
        <h3>Unified write + retrieval loop through the orchestrator</h3>
        <p>
          Every write enters through the orchestrator, which records durable raw data, fans out to specialized stores,
          and continuously protects queue and storage health. Every search comes back through the same orchestrator so
          results can be fused, reranked, and improved over time from feedback.
        </p>
        <ul class="chips">
          <li>Write intake</li>
          <li>Outbox fanout</li>
          <li>Federated search</li>
          <li>Learning rerank</li>
          <li>Retention + guardrails</li>
        </ul>
      </article>
    </section>

    <section class="section">
      <div class="kicker">Service Map</div>
      <h2>Data Flow</h2>
      <div class="grid flow-grid">
        <article class="card flow-card">
          <div class="kicker">Receive + Store</div>
          <h3>Write workflow</h3>
          <ol class="flow-lane">
            <li class="flow-node"><strong>1.</strong> Agents and apps send writes to <code>POST /memory/write</code>.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>2.</strong> Orchestrator validates payloads, assigns metadata, and records queue intent.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>3.</strong> Mongo raw captures durable source-of-truth write history.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>4.</strong> Outbox fanout distributes to Memory Bank, Qdrant, MindsDB, and Letta.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>5.</strong> Retention and telemetry workers enforce storage and queue guardrails.</li>
          </ol>
        </article>
        <article class="card flow-card">
          <div class="kicker">Read + Return</div>
          <h3>Retrieval workflow</h3>
          <ol class="flow-lane">
            <li class="flow-node"><strong>1.</strong> Agents and apps send queries to <code>POST /memory/search</code>.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>2.</strong> Orchestrator runs federated retrieval across Memory Bank, Qdrant, Mongo raw, MindsDB, and Letta.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>3.</strong> Candidates are deduplicated, scored, and reranked with learning signals.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>4.</strong> Best matches return to the caller with source metadata and confidence context.</li>
            <li class="flow-arrow" aria-hidden="true">↓</li>
            <li class="flow-node"><strong>5.</strong> Feedback loops update retrieval preferences so quality improves over time.</li>
          </ol>
        </article>
      </div>
      <div class="grid component-grid">
        <article class="card component-card">
          <h3>Orchestrator</h3>
          <p><strong>Benefit:</strong> one control plane for writes, retrieval, and policy.</p>
          <p><strong>Why:</strong> central coordination is what allows multi-source ranking and learning to compound.</p>
        </article>
        <article class="card component-card">
          <h3>Memory Bank MCP</h3>
          <p><strong>Benefit:</strong> canonical project/file context store.</p>
          <p><strong>Why:</strong> keeps user-facing memory deterministic and compatible with MCP-native clients.</p>
        </article>
        <article class="card component-card">
          <h3>Qdrant</h3>
          <p><strong>Benefit:</strong> high-speed semantic recall.</p>
          <p><strong>Why:</strong> vector retrieval gives broad relevance quickly before deeper reranking.</p>
        </article>
        <article class="card component-card">
          <h3>Mongo Raw</h3>
          <p><strong>Benefit:</strong> durable source-of-truth write ledger.</p>
          <p><strong>Why:</strong> protects recoverability and enables repair/rehydrate workflows.</p>
        </article>
        <article class="card component-card">
          <h3>MindsDB</h3>
          <p><strong>Benefit:</strong> SQL-friendly analytics and structured querying.</p>
          <p><strong>Why:</strong> complements semantic search with tabular and operational insight paths.</p>
        </article>
        <article class="card component-card">
          <h3>Letta (RAG memory)</h3>
          <p><strong>Benefit:</strong> long-horizon archival context for agent reasoning.</p>
          <p><strong>Why:</strong> deep memory context improves difficult recall beyond nearest-neighbor hits.</p>
        </article>
        <article class="card component-card">
          <h3>Fanout Outbox</h3>
          <p><strong>Benefit:</strong> resilient async delivery with retries, coalescing, and admission control.</p>
          <p><strong>Why:</strong> prevents sink instability from breaking ingestion reliability.</p>
        </article>
        <article class="card component-card">
          <h3>Retention + Telemetry</h3>
          <p><strong>Benefit:</strong> bounded storage growth and observable runtime behavior.</p>
          <p><strong>Why:</strong> operational stability is required for learning retrieval to stay trustworthy.</p>
        </article>
      </div>
    </section>

    <section class="section">
      <article class="card">
        <div class="kicker">Why this boosts learning retrieval impact</div>
        <h3>Learning is strongest when memory is both rich and reliable</h3>
        <p>
          The orchestrator's learning schema can only improve ranking if retrieval sources stay healthy, durable,
          and synchronized. This architecture makes that possible: Qdrant provides fast candidates, Letta supplies
          deeper RAG context, Mongo guarantees recovery, MindsDB adds structured recall, and guardrails keep the
          full loop from collapsing under pressure.
        </p>
      </article>
    </section>

    <section class="section">
      <div class="kicker">Flexible Launch</div>
      <h2>Deployment Modes</h2>
      <div class="grid mode-grid">
        <article class="card mode-card">
          <h3>Lite mode</h3>
          <p>Best for local development and constrained laptops where stable memory services matter more than deep analytics.</p>
          <ul class="spec-list">
            <li><strong>Includes:</strong> Orchestrator, Memory Bank MCP, Mongo raw, Qdrant, outbox fanout, retention workers</li>
            <li><strong>Compute:</strong> 2-4 vCPU recommended</li>
            <li><strong>Memory:</strong> 4-8 GB RAM baseline</li>
            <li><strong>Storage:</strong> 25-60 GB SSD depending on write volume</li>
          </ul>
        </article>
        <article class="card mode-card">
          <h3>Full mode</h3>
          <p>Best for high-write workloads and richer retrieval where learning loops use every sink, including RAG through Letta.</p>
          <ul class="spec-list">
            <li><strong>Includes:</strong> Lite mode plus MindsDB analytics, Letta archival memory, observability stack, and full rehydrate tooling</li>
            <li><strong>Compute:</strong> 6-8 vCPU recommended</li>
            <li><strong>Memory:</strong> 16-24 GB RAM baseline</li>
            <li><strong>Storage:</strong> 120-200 GB SSD depending on retention policy</li>
          </ul>
        </article>
      </div>
    </section>

    <footer class="footer">
      <span>Context Lattice public overview</span>
      <span>Private by default. MCP-compatible by design.</span>
    </footer>
  </main>
</body>
</html>
